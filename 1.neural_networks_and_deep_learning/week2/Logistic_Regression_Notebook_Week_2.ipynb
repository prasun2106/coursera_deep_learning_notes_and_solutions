{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data\n",
    "The coursera notebook has a customized module lr_utils to load dataset. But that is not available for everyone. So I have downloaded the dataset from the coursera website and now I will be importing that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = pd.read_csv('data/train_set_x.csv')\n",
    "train_set_y = pd.read_csv('data/train_set_y.csv')\n",
    "test_set_x = pd.read_csv('data/test_set_x.csv')\n",
    "test_set_y = pd.read_csv('data/test_set_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train_set_x,train_set_y,test_set_x,test_set_y]:\n",
    "    data.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209) (1, 209) (12288, 50) (1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_x.shape, train_set_y.shape, test_set_x.shape, test_set_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = np.array(train_set_x)\n",
    "train_set_y = np.array(train_set_y)\n",
    "test_set_x = np.array(test_set_x)\n",
    "test_set_y = np.array(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_parameters(dim):\n",
    "    W = np.zeros(dim) #dimension in this case will be - (n_px*n_px*3,1)\n",
    "    b = 0\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation(W, b, X, Y):\n",
    "    '''finding z, a, L, J, dw, db \n",
    "    required - w,b, X, Y'''\n",
    "    m = X.shape[1]\n",
    "    Z = np.dot(W.T,X) + b #shape (1,m)\n",
    "    A = sigmoid(Z) #shape (1,m)\n",
    "    cost = (1/m)*np.sum(-(Y)*np.log(A) - (1-Y)*(np.log(1-A))) #cost function = average of all loss function.\n",
    "    # we are using normal product (element-wise) because Y and log(A) both have the same dimension and both of them are a number for a training example\n",
    "    \n",
    "    dZ = A-Y\n",
    "    dW = np.dot(X, dZ.T)/m  #dJ/dw = dw = x*dz --> X*(A-Y)\n",
    "    db = dZ.sum()/m # shape (1,1) \n",
    "    \n",
    "    grad = {'dW':dW, 'db':db}\n",
    "    return grad, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(W, b, X, Y, num_iteration, alpha, print_cost = False):\n",
    "    costs = []\n",
    "    for i in range(num_iteration):\n",
    "        \n",
    "        # gradients\n",
    "        grads, cost = propagation(W,b,X,Y)\n",
    "        dW = grads['dW']\n",
    "        db = grads['db']\n",
    "\n",
    "        # update parameters\n",
    "        W = W - alpha*dW\n",
    "        b = b - alpha*db\n",
    "        \n",
    "        # print cost for every 100th iteration\n",
    "        if i%1000 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost == True and i%1000== 0:\n",
    "            print(f\"cost after {i}th iteration:{cost}\")\n",
    "    \n",
    "    params = {'W':W, 'b':b}\n",
    "    grads = {'dW':dW, 'db':db}\n",
    "    \n",
    "    return params, grads, costs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[0.99993216]\n",
      " [1.99980262]]\n",
      "db = 0.49993523062470574\n",
      "cost = 6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "W, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
    "grads, cost = propagation(W, b, X, Y)\n",
    "print (\"dW = \" + str(grads[\"dW\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.1124579 ]\n",
      " [0.23106775]]\n",
      "b = 1.5593049248448891\n",
      "dW = [[0.90158428]\n",
      " [1.76250842]]\n",
      "db = 0.4304620716786828\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimization(W, b, X, Y, num_iteration= 100, alpha= 0.009, print_cost = False)\n",
    "\n",
    "print (\"W = \" + str(params[\"W\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dW = \" + str(grads[\"dW\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0th iteration:6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "params, grads, cost = optimization(W, b, X, Y, 1000, 0.3, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "#     W = W.reshape(X.shape[1],1)\n",
    "    A = sigmoid(np.dot(W.T, X) + b) #the predictions - shape - (1,m)\n",
    "    y_pred = np.zeros((1,A.shape[1]))\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0,i]>0.5:\n",
    "            y_pred[0,i]=1\n",
    "        else:\n",
    "            y_pred[0,i] = 0\n",
    "    \n",
    "    return A, Z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_x, train_y, test_x, test_y, num_iteration, alpha, print_cost = False):\n",
    "    #training our model\n",
    "    #intitialize parameters:\n",
    "    m = train_x.shape[1]\n",
    "    n = train_x.shape[0]\n",
    "    W,b = initilize_parameters((n,1))\n",
    "    \n",
    "    #forward propagation\n",
    "    grad, cost = propagation(W,b,train_x, train_y)\n",
    "    \n",
    "    #extract gradients\n",
    "    dW = grad['dW']\n",
    "    db = grad['db']\n",
    "    \n",
    "    #back propagation:\n",
    "    params, grads, costs = optimization(W,b,train_x, train_y,  num_iteration, alpha, print_cost)\n",
    "    \n",
    "    #extract parameters and gradients\n",
    "    W = params['W']\n",
    "    b = params['b']\n",
    "    dW = grads['dW']\n",
    "    db = grads['db']\n",
    "    \n",
    "    # make prediction:\n",
    "    A, Z, y_predicted_train = predict(W, b, train_x)\n",
    "    A, Z, y_predicted_test = predict(W,b, test_x)\n",
    "    \n",
    "    #accuracy:\n",
    "    differences_train = (y_predicted_train - train_y)\n",
    "    differences_test = (y_predicted_test - test_y)\n",
    "    \n",
    "    correct_prediction_train = (differences_train==0).sum() #number of zeros in differences_train\n",
    "    correct_prediction_test = (differences_test==0).sum() #number of zeros in differences_test\n",
    "    \n",
    "    accuracy_train = correct_prediction_train/train_y.shape[1]\n",
    "    accuracy_test = correct_prediction_test/test_y.shape[1]\n",
    "    \n",
    "    print(accuracy_train, accuracy_test)\n",
    "    return y_predicted_train, y_predicted_test, A, Z, differences_train, differences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0th iteration:0.6931471805599453\n",
      "cost after 1000th iteration:0.45051697876399244\n",
      "cost after 2000th iteration:0.3849149580141818\n",
      "cost after 3000th iteration:0.3406841599116204\n",
      "cost after 4000th iteration:0.307173684015738\n",
      "cost after 5000th iteration:0.28028559698665567\n",
      "cost after 6000th iteration:0.25797225056184836\n",
      "cost after 7000th iteration:0.23903664748435746\n",
      "cost after 8000th iteration:0.2227050315749366\n",
      "cost after 9000th iteration:0.208442664538682\n",
      "0.9808612440191388 0.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]]),\n",
       " array([[0.14986692, 0.19546677, 0.10209523, 0.19047897, 0.10428885,\n",
       "         0.38657037, 0.07833127, 0.21770371, 0.24517565, 0.29715127,\n",
       "         0.28192385, 0.22444749, 0.34411739, 0.10798341, 0.08547993,\n",
       "         0.16617413, 0.18760477, 0.07877522, 0.08708322, 0.20390852,\n",
       "         0.18712999, 0.17481577, 0.16376661, 0.3162946 , 0.27079031,\n",
       "         0.15032136, 0.17862936, 0.2799522 , 0.15446559, 0.16701596,\n",
       "         0.35998496, 0.16447862, 0.15012006, 0.2069859 , 0.16225567,\n",
       "         0.05775751, 0.22740037, 0.21110622, 0.2097195 , 0.17821951,\n",
       "         0.09390503, 0.24074382, 0.12818975, 0.22421689, 0.22710637,\n",
       "         0.13644787, 0.22056562, 0.26029751, 0.19047273, 0.09006719]]),\n",
       " array([[ 9, 12]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.]]),\n",
       " array([[-1., -1., -1., -1., -1.,  0., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          0.,  0., -1.,  0., -1., -1., -1., -1.,  0.,  0., -1., -1., -1.,\n",
       "         -1.,  0., -1.,  0., -1., -1., -1., -1.,  0.,  0.,  0., -1.,  0.,\n",
       "          0., -1., -1., -1.,  0.,  0.,  0., -1., -1., -1.,  0.]]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_set_x,train_set_y,test_set_x, test_set_y,10000,0.003, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs good on training set as the accuracy is around 98%. But the model accuracy on test set is the worst. So, this is the case of overfitting. In the next week's notebook, we will learn concepts like regularisation and hyperparametric tuning to resolve this issue."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
