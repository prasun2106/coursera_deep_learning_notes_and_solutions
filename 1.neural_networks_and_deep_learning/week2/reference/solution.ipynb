{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data\n",
    "The coursera notebook has a customized module lr_utils to load dataset. But that is not available for everyone. So I have downloaded the dataset from the coursera website and now I will be importing that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = pd.read_csv('data/train_set_x.csv')\n",
    "train_set_y = pd.read_csv('data/train_set_y.csv')\n",
    "test_set_x = pd.read_csv('data/test_set_x.csv')\n",
    "test_set_y = pd.read_csv('data/test_set_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train_set_x,train_set_y,test_set_x,test_set_y]:\n",
    "    data.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209) (1, 209) (12288, 50) (1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_x.shape, train_set_y.shape, test_set_x.shape, test_set_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = np.array(train_set_x)\n",
    "train_set_y = np.array(train_set_y)\n",
    "test_set_x = np.array(test_set_x)\n",
    "test_set_y = np.array(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_parameters(dim):\n",
    "    W = np.zeros(dim) #dimension in this case will be - (n_px*n_px*3,1)\n",
    "    b = 0\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation(W, b, X, Y):\n",
    "    '''finding z, a, L, J, dw, db \n",
    "    required - w,b, X, Y'''\n",
    "    m = X.shape[1]\n",
    "    Z = np.dot(W.T,X) + b #shape (1,m)\n",
    "    A = sigmoid(Z) #shape (1,m)\n",
    "    cost = (1/m)*np.sum(-(Y)*np.log(A) - (1-Y)*(np.log(1-A))) #cost function = average of all loss function.\n",
    "    # we are using normal product (element-wise) because Y and log(A) both have the same dimension and both of them are a number for a training example\n",
    "    \n",
    "    dZ = A-Y\n",
    "    dW = np.dot(X, dZ.T)/m  #dJ/dw = dw = x*dz --> X*(A-Y)\n",
    "    db = dZ.sum()/m # shape (1,1) \n",
    "    \n",
    "    grad = {'dW':dW, 'db':db}\n",
    "    return grad, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(W, b, X, Y, num_iteration, alpha, print_cost = False):\n",
    "    costs = []\n",
    "    for i in range(num_iteration):\n",
    "        \n",
    "        # gradients\n",
    "        grads, cost = propagation(W,b,X,Y)\n",
    "        dW = grads['dW']\n",
    "        db = grads['db']\n",
    "\n",
    "        # update parameters\n",
    "        W = W - alpha*dW\n",
    "        b = b - alpha*db\n",
    "        \n",
    "        # print cost for every 100th iteration\n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost == True and i%100 == 0:\n",
    "            print(f\"cost after {i}th iteration:{cost}\")\n",
    "    \n",
    "    params = {'W':W, 'b':b}\n",
    "    grads = {'dW':dW, 'db':db}\n",
    "    \n",
    "    return params, grads, costs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[0.99993216]\n",
      " [1.99980262]]\n",
      "db = 0.49993523062470574\n",
      "cost = 6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "W, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
    "grads, cost = propagation(W, b, X, Y)\n",
    "print (\"dW = \" + str(grads[\"dW\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.1124579 ]\n",
      " [0.23106775]]\n",
      "b = 1.5593049248448891\n",
      "dW = [[0.90158428]\n",
      " [1.76250842]]\n",
      "db = 0.4304620716786828\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimization(W, b, X, Y, num_iteration= 100, alpha= 0.009, print_cost = False)\n",
    "\n",
    "print (\"W = \" + str(params[\"W\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dW = \" + str(grads[\"dW\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0th iteration:6.000064773192205\n",
      "cost after 100th iteration:0.2724906479359414\n",
      "cost after 200th iteration:0.16610064610494046\n",
      "cost after 300th iteration:0.11701587919814488\n",
      "cost after 400th iteration:0.08956095644802074\n",
      "cost after 500th iteration:0.07223764238667835\n",
      "cost after 600th iteration:0.060387871321805675\n",
      "cost after 700th iteration:0.051803485634201854\n",
      "cost after 800th iteration:0.04531342894137105\n",
      "cost after 900th iteration:0.04024254394675909\n"
     ]
    }
   ],
   "source": [
    "params, grads, cost = optimization(W, b, X, Y, 1000, 0.3, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "#     W = W.reshape(X.shape[1],1)\n",
    "    A = sigmoid(np.dot(W.T, X) + b) #the predictions - shape - (1,m)\n",
    "    y_pred = np.zeros((1,A.shape[1]))\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0,i]>0.5:\n",
    "            y_pred[0,i]=1\n",
    "        else:\n",
    "            y_pred[0,i] = 0\n",
    "    \n",
    "    return A, Z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_x, train_y, test_x, test_y, num_iteration, alpha, print_cost = False):\n",
    "    #training our model\n",
    "    #intitialize parameters:\n",
    "    m = train_x.shape[1]\n",
    "    n = train_x.shape[0]\n",
    "    W,b = initilize_parameters((n,1))\n",
    "    \n",
    "    #forward propagation\n",
    "    grad, cost = propagation(W,b,train_x, train_y)\n",
    "    \n",
    "    #extract gradients\n",
    "    dW = grad['dW']\n",
    "    db = grad['db']\n",
    "    \n",
    "    #back propagation:\n",
    "    params, grads, costs = optimization(W,b,train_x, train_y,  num_iteration, alpha, print_cost)\n",
    "    \n",
    "    #extract parameters and gradients\n",
    "    W = params['W']\n",
    "    b = params['b']\n",
    "    dW = grads['dW']\n",
    "    db = grads['db']\n",
    "    \n",
    "    # make prediction:\n",
    "    A, Z, y_predicted_train = predict(W, b, train_x)\n",
    "    A, Z, y_predicted_test = predict(W,b, test_x)\n",
    "    \n",
    "    #accuracy:\n",
    "    differences_train = (y_predicted_train - train_y)\n",
    "    differences_test = (y_predicted_test - test_y)\n",
    "    \n",
    "    correct_prediction_train = (differences_train==0).sum() #number of zeros in differences_train\n",
    "    correct_prediction_test = (differences_test==0).sum() #number of zeros in differences_test\n",
    "    \n",
    "    accuracy_train = correct_prediction_train/train_y.shape[1]\n",
    "    accuracy_test = correct_prediction_test/test_y.shape[1]\n",
    "    \n",
    "    print(accuracy_train, accuracy_test)\n",
    "    return y_predicted_train, y_predicted_test, A, Z, differences_train, differences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0th iteration:0.6931471805599453\n",
      "cost after 100th iteration:0.6254741983805133\n",
      "cost after 200th iteration:0.6150649153206877\n",
      "cost after 300th iteration:0.6078426806708578\n",
      "cost after 400th iteration:0.6020920760315909\n",
      "cost after 500th iteration:0.5971430946044409\n",
      "cost after 600th iteration:0.5927029668239371\n",
      "cost after 700th iteration:0.5886264772768176\n",
      "cost after 800th iteration:0.584832043386549\n",
      "cost after 900th iteration:0.5812685476354333\n",
      "cost after 1000th iteration:0.577900997177046\n",
      "cost after 1100th iteration:0.5747037258071908\n",
      "cost after 1200th iteration:0.5716568860328716\n",
      "cost after 1300th iteration:0.5687445027318226\n",
      "cost after 1400th iteration:0.5659533257226419\n",
      "cost after 1500th iteration:0.5632721170511988\n",
      "cost after 1600th iteration:0.5606911868236855\n",
      "cost after 1700th iteration:0.5582020771188063\n",
      "cost after 1800th iteration:0.5557973374409302\n",
      "cost after 1900th iteration:0.5534703588162203\n",
      "cost after 2000th iteration:0.5512152468291814\n",
      "cost after 2100th iteration:0.5490267214726007\n",
      "cost after 2200th iteration:0.5469000361356909\n",
      "cost after 2300th iteration:0.5448309107279758\n",
      "cost after 2400th iteration:0.5428154755758273\n",
      "cost after 2500th iteration:0.5408502237572332\n",
      "cost after 2600th iteration:0.5389319702019417\n",
      "cost after 2700th iteration:0.5370578163210779\n",
      "cost after 2800th iteration:0.5352251192271691\n",
      "cost after 2900th iteration:0.5334314648130916\n",
      "cost after 3000th iteration:0.5316746441078095\n",
      "cost after 3100th iteration:0.5299526324372309\n",
      "cost after 3200th iteration:0.5282635710022611\n",
      "cost after 3300th iteration:0.5266057505510904\n",
      "cost after 3400th iteration:0.524977596874119\n",
      "cost after 3500th iteration:0.5233776578912371\n",
      "cost after 3600th iteration:0.5218045921348519\n",
      "cost after 3700th iteration:0.5202571584598682\n",
      "cost after 3800th iteration:0.5187342068349812\n",
      "cost after 3900th iteration:0.5172346700891093\n",
      "cost after 4000th iteration:0.5157575565032653\n",
      "cost after 4100th iteration:0.5143019431521806\n",
      "cost after 4200th iteration:0.512866969911987\n",
      "cost after 4300th iteration:0.5114518340605665\n",
      "cost after 4400th iteration:0.5100557854060672\n",
      "cost after 4500th iteration:0.5086781218867765\n",
      "cost after 4600th iteration:0.5073181855922194\n",
      "cost after 4700th iteration:0.5059753591611703\n",
      "cost after 4800th iteration:0.504649062517336\n",
      "cost after 4900th iteration:0.5033387499079126\n",
      "cost after 5000th iteration:0.5020439072141097\n",
      "cost after 5100th iteration:0.5007640495061548\n",
      "cost after 5200th iteration:0.49949871881829916\n",
      "cost after 5300th iteration:0.49824748212200287\n",
      "cost after 5400th iteration:0.4970099294778146\n",
      "cost after 5500th iteration:0.4957856723485342\n",
      "cost after 5600th iteration:0.49457434205808215\n",
      "cost after 5700th iteration:0.4933755883821239\n",
      "cost after 5800th iteration:0.49218907825794167\n",
      "cost after 5900th iteration:0.49101449460233104\n",
      "cost after 6000th iteration:0.48985153522744057\n",
      "cost after 6100th iteration:0.4886999118454909\n",
      "cost after 6200th iteration:0.4875593491542179\n",
      "cost after 6300th iteration:0.4864295839956955\n",
      "cost after 6400th iteration:0.485310364581915\n",
      "cost after 6500th iteration:0.4842014497811518\n",
      "cost after 6600th iteration:0.48310260845972214\n",
      "cost after 6700th iteration:0.4820136188742616\n",
      "cost after 6800th iteration:0.48093426811011586\n",
      "cost after 6900th iteration:0.4798643515618566\n",
      "cost after 7000th iteration:0.4788036724523111\n",
      "cost after 7100th iteration:0.4777520413868326\n",
      "cost after 7200th iteration:0.47670927593983947\n",
      "cost after 7300th iteration:0.475675200270932\n",
      "cost after 7400th iteration:0.47464964476813415\n",
      "cost after 7500th iteration:0.47363244571603835\n",
      "cost after 7600th iteration:0.4726234449868254\n",
      "cost after 7700th iteration:0.47162248975232013\n",
      "cost after 7800th iteration:0.4706294322153997\n",
      "cost after 7900th iteration:0.4696441293592266\n",
      "cost after 8000th iteration:0.4686664427129105\n",
      "cost after 8100th iteration:0.4676962381323206\n",
      "cost after 8200th iteration:0.4667333855948878\n",
      "cost after 8300th iteration:0.4657777590073303\n",
      "cost after 8400th iteration:0.4648292360253285\n",
      "cost after 8500th iteration:0.46388769788425727\n",
      "cost after 8600th iteration:0.46295302924016046\n",
      "cost after 8700th iteration:0.46202511802021695\n",
      "cost after 8800th iteration:0.4611038552820112\n",
      "cost after 8900th iteration:0.46018913508097997\n",
      "cost after 9000th iteration:0.459280854345452\n",
      "cost after 9100th iteration:0.45837891275875153\n",
      "cost after 9200th iteration:0.4574832126478741\n",
      "cost after 9300th iteration:0.45659365887828207\n",
      "cost after 9400th iteration:0.4557101587544076\n",
      "cost after 9500th iteration:0.4548326219254764\n",
      "cost after 9600th iteration:0.45396096029630123\n",
      "cost after 9700th iteration:0.4530950879427174\n",
      "cost after 9800th iteration:0.45223492103135976\n",
      "cost after 9900th iteration:0.4513803777435006\n",
      "0.7751196172248804 0.34\n"
     ]
    }
   ],
   "source": [
    "y_predicted_train, y_predicted_test, A, Z, differences_train, differences_test = model(train_set_x,train_set_y,test_set_x, test_set_y,10000,0.0003, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0th iteration:0.6931471805599453\n",
      "cost after 100th iteration:4.1951486381215926\n",
      "cost after 200th iteration:5.703759087552092\n",
      "cost after 300th iteration:0.9896755997164715\n",
      "cost after 400th iteration:0.9440968925860884\n",
      "cost after 500th iteration:5.269416743561517\n",
      "cost after 600th iteration:2.201651664181324\n",
      "cost after 700th iteration:8.014781374937032\n",
      "cost after 800th iteration:2.2335112863425053\n",
      "cost after 900th iteration:0.5001056197582409\n",
      "cost after 1000th iteration:3.6667542256139103\n",
      "cost after 1100th iteration:1.1085188824310679\n",
      "cost after 1200th iteration:1.7236486826168456\n",
      "cost after 1300th iteration:3.4964051831886898\n",
      "cost after 1400th iteration:3.2363556111731895\n",
      "cost after 1500th iteration:3.0347657447366\n",
      "cost after 1600th iteration:2.86015503359166\n",
      "cost after 1700th iteration:2.7091137057069776\n",
      "cost after 1800th iteration:2.5762822178646276\n",
      "cost after 1900th iteration:2.454928927084223\n",
      "cost after 2000th iteration:2.3386624372685927\n",
      "cost after 2100th iteration:2.222307868547638\n",
      "cost after 2200th iteration:2.1034982350589595\n",
      "cost after 2300th iteration:1.978906355662469\n",
      "cost after 2400th iteration:1.8336344277407002\n",
      "cost after 2500th iteration:1.5987663444140852\n",
      "cost after 2600th iteration:3.297140096183435\n",
      "cost after 2700th iteration:1.4411332256957097\n",
      "cost after 2800th iteration:2.0168261306752027\n",
      "cost after 2900th iteration:2.437619206353715\n",
      "cost after 3000th iteration:0.3447721911684918\n",
      "cost after 3100th iteration:0.2813160947049877\n",
      "cost after 3200th iteration:2.4760495828191766\n",
      "cost after 3300th iteration:0.20224423640467107\n",
      "cost after 3400th iteration:2.143258434712221\n",
      "cost after 3500th iteration:0.12043733290019526\n",
      "cost after 3600th iteration:0.06589222900269052\n",
      "cost after 3700th iteration:0.054567425619755576\n",
      "cost after 3800th iteration:0.04720356164496276\n",
      "cost after 3900th iteration:0.041820681959297785\n",
      "cost after 4000th iteration:0.03781014498373904\n",
      "cost after 4100th iteration:0.03475618606550411\n",
      "cost after 4200th iteration:0.03236817527103969\n",
      "cost after 4300th iteration:0.030448671770075146\n",
      "cost after 4400th iteration:0.028865749101542867\n",
      "cost after 4500th iteration:0.027531014898800412\n",
      "cost after 4600th iteration:0.026384293361623273\n",
      "cost after 4700th iteration:0.025383676818845072\n",
      "cost after 4800th iteration:0.02449923627845732\n",
      "cost after 4900th iteration:0.023709053970085405\n",
      "cost after 5000th iteration:0.022996693114044606\n",
      "cost after 5100th iteration:0.02234955574896648\n",
      "cost after 5200th iteration:0.021757794840442705\n",
      "cost after 5300th iteration:0.021213577908997416\n",
      "cost after 5400th iteration:0.020710577800948555\n",
      "cost after 5500th iteration:0.020243613165752274\n",
      "cost after 5600th iteration:0.019808389585462313\n",
      "cost after 5700th iteration:0.019401309706239843\n",
      "cost after 5800th iteration:0.01901933156916366\n",
      "cost after 5900th iteration:0.018659861215258516\n",
      "cost after 6000th iteration:0.018320670076845808\n",
      "cost after 6100th iteration:0.01799983057965099\n",
      "cost after 6200th iteration:0.017695665323903903\n",
      "cost after 6300th iteration:0.017406706531215028\n",
      "cost after 6400th iteration:0.01713166335243037\n",
      "cost after 6500th iteration:0.016869395266894724\n",
      "cost after 6600th iteration:0.016618890254065998\n",
      "cost after 6700th iteration:0.016379246742257124\n",
      "cost after 6800th iteration:0.016149658575036772\n",
      "cost after 6900th iteration:0.015929402409531886\n",
      "cost after 7000th iteration:0.015717827090338993\n",
      "cost after 7100th iteration:0.015514344640279865\n",
      "cost after 7200th iteration:0.015318422583457333\n",
      "cost after 7300th iteration:0.015129577373095764\n",
      "cost after 7400th iteration:0.014947368740864112\n",
      "cost after 7500th iteration:0.01477139481895553\n",
      "cost after 7600th iteration:0.014601287913450889\n",
      "cost after 7700th iteration:0.014436710829142425\n",
      "cost after 7800th iteration:0.014277353663312056\n",
      "cost after 7900th iteration:0.014122930999907312\n",
      "cost after 8000th iteration:0.013973179446863977\n",
      "cost after 8100th iteration:0.013827855468542826\n",
      "cost after 8200th iteration:0.013686733472807272\n",
      "cost after 8300th iteration:0.013549604118501042\n",
      "cost after 8400th iteration:0.013416272814245327\n",
      "cost after 8500th iteration:0.013286558383773439\n",
      "cost after 8600th iteration:0.013160291876611413\n",
      "cost after 8700th iteration:0.013037315505927254\n",
      "cost after 8800th iteration:0.012917481697912863\n",
      "cost after 8900th iteration:0.012800652239208872\n",
      "cost after 9000th iteration:0.01268669751070644\n",
      "cost after 9100th iteration:0.012575495797610144\n",
      "cost after 9200th iteration:0.012466932666970935\n",
      "cost after 9300th iteration:0.012360900405031376\n",
      "cost after 9400th iteration:0.012257297507698993\n",
      "cost after 9500th iteration:0.012156028218299964\n",
      "cost after 9600th iteration:0.012057002107489041\n",
      "cost after 9700th iteration:0.011960133690814783\n",
      "cost after 9800th iteration:0.011865342079980561\n",
      "cost after 9900th iteration:0.01177255066431155\n",
      "1.0 0.34\n"
     ]
    }
   ],
   "source": [
    "y_predicted_train, y_predicted_test, A, Z, differences_train, differences_test = model(train_set_x,train_set_y,test_set_x, test_set_y,10000,0.03, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw  # need to broadcast\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.709726\n",
      "Cost after iteration 200: 0.657712\n",
      "Cost after iteration 300: 0.614611\n",
      "Cost after iteration 400: 0.578001\n",
      "Cost after iteration 500: 0.546372\n",
      "Cost after iteration 600: 0.518331\n",
      "Cost after iteration 700: 0.492852\n",
      "Cost after iteration 800: 0.469259\n",
      "Cost after iteration 900: 0.447139\n",
      "Cost after iteration 1000: 0.426262\n",
      "Cost after iteration 1100: 0.406617\n",
      "Cost after iteration 1200: 0.388723\n",
      "Cost after iteration 1300: 0.374678\n",
      "Cost after iteration 1400: 0.365826\n",
      "Cost after iteration 1500: 0.358532\n",
      "Cost after iteration 1600: 0.351612\n",
      "Cost after iteration 1700: 0.345012\n",
      "Cost after iteration 1800: 0.338704\n",
      "Cost after iteration 1900: 0.332664\n",
      "Cost after iteration 2000: 0.326870\n",
      "Cost after iteration 2100: 0.321305\n",
      "Cost after iteration 2200: 0.315951\n",
      "Cost after iteration 2300: 0.310795\n",
      "Cost after iteration 2400: 0.305822\n",
      "Cost after iteration 2500: 0.301023\n",
      "Cost after iteration 2600: 0.296386\n",
      "Cost after iteration 2700: 0.291901\n",
      "Cost after iteration 2800: 0.287560\n",
      "Cost after iteration 2900: 0.283354\n",
      "Cost after iteration 3000: 0.279278\n",
      "Cost after iteration 3100: 0.275323\n",
      "Cost after iteration 3200: 0.271485\n",
      "Cost after iteration 3300: 0.267756\n",
      "Cost after iteration 3400: 0.264132\n",
      "Cost after iteration 3500: 0.260608\n",
      "Cost after iteration 3600: 0.257179\n",
      "Cost after iteration 3700: 0.253842\n",
      "Cost after iteration 3800: 0.250591\n",
      "Cost after iteration 3900: 0.247425\n",
      "Cost after iteration 4000: 0.244338\n",
      "Cost after iteration 4100: 0.241327\n",
      "Cost after iteration 4200: 0.238391\n",
      "Cost after iteration 4300: 0.235525\n",
      "Cost after iteration 4400: 0.232727\n",
      "Cost after iteration 4500: 0.229994\n",
      "Cost after iteration 4600: 0.227325\n",
      "Cost after iteration 4700: 0.224716\n",
      "Cost after iteration 4800: 0.222166\n",
      "Cost after iteration 4900: 0.219672\n",
      "Cost after iteration 5000: 0.217233\n",
      "Cost after iteration 5100: 0.214846\n",
      "Cost after iteration 5200: 0.212510\n",
      "Cost after iteration 5300: 0.210224\n",
      "Cost after iteration 5400: 0.207985\n",
      "Cost after iteration 5500: 0.205791\n",
      "Cost after iteration 5600: 0.203643\n",
      "Cost after iteration 5700: 0.201538\n",
      "Cost after iteration 5800: 0.199474\n",
      "Cost after iteration 5900: 0.197451\n",
      "Cost after iteration 6000: 0.195467\n",
      "Cost after iteration 6100: 0.193522\n",
      "Cost after iteration 6200: 0.191614\n",
      "Cost after iteration 6300: 0.189741\n",
      "Cost after iteration 6400: 0.187903\n",
      "Cost after iteration 6500: 0.186100\n",
      "Cost after iteration 6600: 0.184329\n",
      "Cost after iteration 6700: 0.182591\n",
      "Cost after iteration 6800: 0.180883\n",
      "Cost after iteration 6900: 0.179206\n",
      "Cost after iteration 7000: 0.177559\n",
      "Cost after iteration 7100: 0.175940\n",
      "Cost after iteration 7200: 0.174349\n",
      "Cost after iteration 7300: 0.172786\n",
      "Cost after iteration 7400: 0.171249\n",
      "Cost after iteration 7500: 0.169738\n",
      "Cost after iteration 7600: 0.168252\n",
      "Cost after iteration 7700: 0.166791\n",
      "Cost after iteration 7800: 0.165354\n",
      "Cost after iteration 7900: 0.163940\n",
      "Cost after iteration 8000: 0.162549\n",
      "Cost after iteration 8100: 0.161181\n",
      "Cost after iteration 8200: 0.159834\n",
      "Cost after iteration 8300: 0.158508\n",
      "Cost after iteration 8400: 0.157204\n",
      "Cost after iteration 8500: 0.155919\n",
      "Cost after iteration 8600: 0.154654\n",
      "Cost after iteration 8700: 0.153409\n",
      "Cost after iteration 8800: 0.152182\n",
      "Cost after iteration 8900: 0.150974\n",
      "Cost after iteration 9000: 0.149784\n",
      "Cost after iteration 9100: 0.148612\n",
      "Cost after iteration 9200: 0.147457\n",
      "Cost after iteration 9300: 0.146319\n",
      "Cost after iteration 9400: 0.145197\n",
      "Cost after iteration 9500: 0.144092\n",
      "Cost after iteration 9600: 0.143002\n",
      "Cost after iteration 9700: 0.141928\n",
      "Cost after iteration 9800: 0.140869\n",
      "Cost after iteration 9900: 0.139825\n",
      "train accuracy: 100.0 %\n",
      "test accuracy: 34.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 10000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initilize_parameters(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimization(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0th iteration:0.6931471805599453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-1302dd8ddd53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-189-e83f0b1c24ce>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Gradient descent (≈ 1 line of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Retrieve parameters w and b from dictionary \"parameters\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-146-3b6a242f07de>\u001b[0m in \u001b[0;36moptimization\u001b[1;34m(W, b, X, Y, num_iteration, alpha, print_cost)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
